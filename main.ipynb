{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import librosa\n",
    "import numpy as np\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_to_spectrogram(audio_path, sr = 22050, n_fft = 2048, hop_length = 512):\n",
    "    y, sr = librosa.load(audio_path,sr = sr)\n",
    "    spectrogram = librosa.stft(y, n_fft = n_fft, hop_length = hop_length)\n",
    "    magnitude = np.abs(spectrogram)\n",
    "    return torch.tensor(magnitude).to(device), sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GramMatrix(input):\n",
    "    # A = batch\n",
    "    # B = Number of feature maps\n",
    "    # C,D = Dimension of feature maps\n",
    "    a,b,c,d = input.size()\n",
    "\n",
    "    features = input.view(a*b,c*d)\n",
    "\n",
    "    G = torch.mm(features, features.t())\n",
    "\n",
    "    #Normalize by dividing by total\n",
    "    return G.div(a*b*c*d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xkhai\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\xkhai\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to C:\\Users\\xkhai/.cache\\torch\\hub\\checkpoints\\vgg19-dcbb9e9d.pth\n",
      "100%|██████████| 548M/548M [01:53<00:00, 5.08MB/s] \n"
     ]
    }
   ],
   "source": [
    "vgg = models.vgg19(pretrained=True).features.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in vgg.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(x, model, layers):\n",
    "    features = []\n",
    "    for i, layer in enumerate(model):\n",
    "        x = layer(x)\n",
    "        if i in layers:\n",
    "            features.append(x)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_loss(content_grams, style_grams):\n",
    "    loss = 0\n",
    "    for content_gram, style_gram in zip(content_grams, style_grams):\n",
    "        loss += F.mse_loss(content_gram, style_gram)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dims):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.linear1 = nn.Linear()\n",
    "        self.linear2 = nn.Linear()\n",
    "\n",
    "        self.N = torch.distributions.Normal(0,1)\n",
    "        self.N.loc = self.N.loc.cuda()\n",
    "        self.N.scale = self.N.scale.cuda()\n",
    "        self.kl = 0\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = torch.flatten(x,dim)\n",
    "        x = F.leaky_relu(self.linear1)\n",
    "        return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class Decoder(nn.module):\n",
    "    def __init__(self,latent_dims):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.linear1 = nn.Linear(latent_dims,512)\n",
    "        self.linear2 = nn.Linear(512,1024)\n",
    "    \n",
    "    def forward(self,z):\n",
    "        z = F.leaky_relu(self.linear1(z))\n",
    "        z = torch.sigmoid(self.linear2(z))\n",
    "        return original_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self,latent_dims):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = Encoder(latent_dims)\n",
    "        self.decoder = Decoder(latent_dims)\n",
    "\n",
    "    def forward(self,x):\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def train(autoencoder,data,epoch = 20):\n",
    "    opt = torch.optim.adamw(autoencoder.parameters())\n",
    "    for epoch in range(epoch):\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
